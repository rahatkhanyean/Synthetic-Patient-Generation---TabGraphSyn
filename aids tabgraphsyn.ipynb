{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf8cd967-0179-464d-a3aa-ec8cd4d3c421",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UCI metadata (short) ===\n",
      "{'name': 'AIDS Clinical Trials Group Study 175'}\n",
      "=== Variables (head) ===\n",
      "     name     role     type demographic  \\\n",
      "0  pidnum       ID  Integer        None   \n",
      "1     cid   Target   Binary        None   \n",
      "2    time  Feature  Integer        None   \n",
      "3     trt  Feature  Integer        None   \n",
      "4     age  Feature  Integer         Age   \n",
      "\n",
      "                                         description units missing_values  \n",
      "0                                         Patient ID  None             no  \n",
      "1   censoring indicator (1 = failure, 0 = censoring)  None             no  \n",
      "2                       time to failure or censoring  None             no  \n",
      "3  treatment indicator (0 = ZDV only; 1 = ZDV + d...  None             no  \n",
      "4                              age (yrs) at baseline  None             no  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/numpy/lib/function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/numpy/lib/function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "/share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/numpy/lib/function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/numpy/lib/function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AIDS (ACTG175) — FINAL METRICS (marginal-aware) ===\n",
      "Marginal KS mean %               5.163132\n",
      "Pairwise |Δρ| mean %             3.427685\n",
      "Detection Acc (%) — Logistic    71.495327\n",
      "Detection AUC — Logistic         0.721616\n",
      "Detection Acc (%) — Best-of     99.579166\n",
      "Detection AUC — Best-of          0.999927\n",
      "Best-of attacker                   RF-400\n",
      "dtype: object\n",
      "Saved:\n",
      " • Synthetic CSV: /data/user/home/rkhan5/AIDS/outputs_aids/synthetic_aids_with_survival.csv\n",
      " • UMAP: /data/user/home/rkhan5/AIDS/outputs_aids/umap_aids_paperstyle.png\n",
      " • Heatmaps: /data/user/home/rkhan5/AIDS/outputs_aids/corr_heatmaps_aids_labeled.png\n",
      " • Table II: /data/user/home/rkhan5/AIDS/outputs_aids/table2_aids.png\n",
      " • Marginal per-variable (CSV): /data/user/home/rkhan5/AIDS/outputs_aids/marginal_errors_per_variable_aids.csv\n",
      " • Pairwise errors (CSV): /data/user/home/rkhan5/AIDS/outputs_aids/pairwise_corr_errors_aids.csv\n",
      " • Survival KM: /data/user/home/rkhan5/AIDS/outputs_aids/survival_km_aids.png\n",
      " • Cox confusion: /data/user/home/rkhan5/AIDS/outputs_aids/table5_aids_cox_confusion.png\n",
      " • Detection Logistic: /data/user/home/rkhan5/AIDS/outputs_aids/table6_aids_detection_logreg.png\n",
      " • Detection Best-of: /data/user/home/rkhan5/AIDS/outputs_aids/table6_aids_detection_bestof.png\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# TabGraphSyn (GCN → VAE → latent DDPM) — ACTG175 (AIDS)\n",
    "# Source: UCI via ucimlrepo ONLY (id=890) — no local CSV\n",
    "#\n",
    "# Goal: Lower marginal distribution error (KS%) without overfitting\n",
    "# Changes:\n",
    "#   • VAE training: KL warm-up + Marginal Distribution Loss (batch-quantile L1)\n",
    "#     + Moment loss (mean/std alignment) to better match 1D marginals\n",
    "#   • Post-decode calibration (holdout-aware):\n",
    "#       - Binary cols: top-k rounding with prevalence shrinkage\n",
    "#       - Continuous cols: partial rank-preserving quantile mapping (α_marg)\n",
    "#   • Metrics now computed AFTER all calibrations (fixes earlier time-metric drift)\n",
    "#\n",
    "# Run:\n",
    "#   pip install -U ucimlrepo numpy pandas torch scikit-learn scipy matplotlib umap-learn statsmodels\n",
    "#   python tabgraphsyn_aids_ucirepo.py\n",
    "#   (or) python tabgraphsyn_aids_ucirepo.py --uci_id 890\n",
    "# ================================================================\n",
    "\n",
    "import os, math, random, argparse, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from umap import UMAP\n",
    "except Exception:\n",
    "    import umap.umap_ as _umap\n",
    "    UMAP = _umap.UMAP\n",
    "\n",
    "# Survival (NO lifelines): statsmodels\n",
    "from statsmodels.duration.survfunc import SurvfuncRight\n",
    "from statsmodels.duration.hazard_regression import PHReg\n",
    "import statsmodels.api as sm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ---------------------------\n",
    "# Reproducibility & device\n",
    "# ---------------------------\n",
    "SEED = 123\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------------------------\n",
    "# Output dir\n",
    "# ---------------------------\n",
    "OUTDIR = os.path.abspath(\"outputs_aids\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# TUNING KNOBS (KS% ↓)\n",
    "# ---------------------------\n",
    "# VAE training\n",
    "VAE_Z        = 40\n",
    "VAE_H        = 320\n",
    "VAE_LR       = 2e-3\n",
    "VAE_WD       = 5e-4\n",
    "VAE_EPOCHS   = 160        # ↑ a bit for better marginals\n",
    "VAE_BS       = 128\n",
    "BETA_MAX     = 0.004      # final KL weight; warm-up from 0 → BETA_MAX\n",
    "MDE_W        = 0.08       # weight for marginal-quantile loss (per-feature)\n",
    "MOMENT_W     = 0.12       # weight for mean/std alignment\n",
    "\n",
    "# GCN\n",
    "GCN_EPOCHS   = 120\n",
    "GCN_LR       = 1e-3\n",
    "GCN_WD       = 1e-3\n",
    "KNN_K        = 10\n",
    "\n",
    "# DDPM\n",
    "TSTEPS       = 450\n",
    "EPS_LR       = 2e-3\n",
    "EPS_WD       = 1e-4\n",
    "DDPM_EPOCHS  = 160\n",
    "COND_NOISE   = 0.02\n",
    "DROP_UNCOND  = 0.10\n",
    "CFG_W        = 2.2\n",
    "TAU_NOISE    = 0.95\n",
    "\n",
    "# Post-decoding alignment\n",
    "CORAL_REG    = 2e-3\n",
    "CORAL_BLEND  = 0.85       # 0=no CORAL, 1=full CORAL, blend in standardized space\n",
    "JITTER_SD    = 0.003\n",
    "\n",
    "# Binary calibration (after inverse-transform, excluding event/time)\n",
    "BIN_SHRINK   = 0.65       # convex blend toward real prevalence (0=no change, 1=match real)\n",
    "# Marginal quantile blend (non-binary, excluding time/event)\n",
    "ALPHA_MARG   = 0.55       # 0=no mapping, 1=full quantile map to real calib\n",
    "HOLDOUT_MARG = 0.20       # portion of real data NOT used for marginal calibr.\n",
    "\n",
    "# Survival calibration\n",
    "LAMBDA_RATE  = 0.85       # shrink event rate toward real\n",
    "ALPHA_TIME   = 0.85       # partial quantile map on times\n",
    "HOLDOUT_SURV = 0.10       # bigger holdout reduces overfitting\n",
    "\n",
    "# ---------------------------\n",
    "# Models\n",
    "# ---------------------------\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=96, emb=48, classes=2, dropout=0.15):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, emb)\n",
    "        self.cls = nn.Linear(emb, classes)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "        self.n1 = nn.LayerNorm(hidden)\n",
    "        self.n2 = nn.LayerNorm(emb)\n",
    "    def layer(self, A, X, W, N, act=True):\n",
    "        H = A @ X\n",
    "        H = self.do(W(H))\n",
    "        H = N(H)\n",
    "        return F.relu(H) if act else H\n",
    "    def forward(self, A, X):\n",
    "        h1 = self.layer(A, X, self.fc1, self.n1, True)\n",
    "        h2 = self.layer(A, h1, self.fc2, self.n2, True)\n",
    "        return self.cls(h2), h2\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_dim, z_dim=40, hidden=320):\n",
    "        super().__init__()\n",
    "        self.e1 = nn.Linear(in_dim, hidden)\n",
    "        self.e2 = nn.Linear(hidden, hidden)\n",
    "        self.mu = nn.Linear(hidden, z_dim)\n",
    "        self.lv = nn.Linear(hidden, z_dim)\n",
    "        self.d1 = nn.Linear(z_dim, hidden)\n",
    "        self.d2 = nn.Linear(hidden, hidden)\n",
    "        self.out = nn.Linear(hidden, in_dim)\n",
    "        self.bn_e1 = nn.LayerNorm(hidden)\n",
    "        self.bn_e2 = nn.LayerNorm(hidden)\n",
    "        self.bn_d1 = nn.LayerNorm(hidden)\n",
    "        self.bn_d2 = nn.LayerNorm(hidden)\n",
    "        self.do = nn.Dropout(0.05)\n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.bn_e1(self.e1(x)))\n",
    "        h = F.relu(self.bn_e2(self.do(self.e2(h))))\n",
    "        return self.mu(h), self.lv(h)\n",
    "    def reparam(self, mu, logv):\n",
    "        std = torch.exp(0.5*logv)\n",
    "        return mu + std * torch.randn_like(std)\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.bn_d1(self.d1(z)))\n",
    "        h = F.relu(self.bn_d2(self.do(self.d2(h))))\n",
    "        return self.out(h)\n",
    "    def forward(self, x, beta=1.0):\n",
    "        mu, lv = self.encode(x)\n",
    "        z = self.reparam(mu, lv)\n",
    "        rec = self.decode(z)\n",
    "        # MSE recon + KL; extra losses handled outside for batch-level stats\n",
    "        rl = F.mse_loss(rec, x, reduction='mean')\n",
    "        kld = -0.5 * torch.mean(1 + lv - mu.pow(2) - lv.exp())\n",
    "        return rec, mu, lv, z, rl, kld\n",
    "\n",
    "def t_embedding(t, dim=64, T=450):\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(torch.linspace(math.log(1.0), math.log(10000.0), steps=half, device=device))\n",
    "    args = (t[:, None].float() / T) * freqs[None, :]\n",
    "    return torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "\n",
    "class EpsNet(nn.Module):\n",
    "    def __init__(self, z_dim, c_dim, t_dim=64, hidden=288):\n",
    "        super().__init__()\n",
    "        self.t_dim = t_dim\n",
    "        self.fc1 = nn.Linear(z_dim + c_dim + t_dim, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, z_dim)\n",
    "        self.do  = nn.Dropout(0.05)\n",
    "        self.nrm = nn.LayerNorm(hidden)\n",
    "    def forward(self, x, c, t, T=450):\n",
    "        te = t_embedding(t, self.t_dim, T)\n",
    "        h = torch.cat([x, c, te], 1)\n",
    "        h = self.do(F.silu(self.fc1(h)))\n",
    "        h = self.do(F.silu(self.nrm(self.fc2(h))))\n",
    "        return self.fc3(h)\n",
    "\n",
    "# ---------------------------\n",
    "# Metrics\n",
    "# ---------------------------\n",
    "def marginal_KS_percent(real_std, synth_std, feat_names):\n",
    "    rows = []\n",
    "    for j in range(real_std.shape[1]):\n",
    "        r = real_std[:, j]; s = synth_std[:, j]\n",
    "        ks = ks_2samp(r, s, alternative='two-sided', mode='asymp').statistic\n",
    "        rows.append({\"feature\": feat_names[j], \"KS%\": 100.0*float(ks)})\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df, float(df[\"KS%\"].mean())\n",
    "\n",
    "def pairwise_corr_error_percent(real_std, synth_std, feat_names):\n",
    "    rows = []\n",
    "    p = real_std.shape[1]\n",
    "    for i in range(p):\n",
    "        for j in range(i+1, p):\n",
    "            r_corr = np.corrcoef(real_std[:, [i, j]].T)[0,1]\n",
    "            s_corr = np.corrcoef(synth_std[:, [i, j]].T)[0,1]\n",
    "            rows.append({\"feat_i\": feat_names[i], \"feat_j\": feat_names[j], \"|Δρ|%\": 100.0*float(abs(s_corr - r_corr))})\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df, float(df[\"|Δρ|%\"].mean())\n",
    "\n",
    "def detection_score_logistic(real_std, synth_std):\n",
    "    n = min(len(real_std), len(synth_std))\n",
    "    X = np.vstack([real_std[:n], synth_std[:n]])\n",
    "    y = np.hstack([np.zeros(n, dtype=int), np.ones(n, dtype=int)])\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=SEED, stratify=y)\n",
    "    clf = LogisticRegression(max_iter=5000, solver=\"lbfgs\")\n",
    "    clf.fit(Xtr, ytr)\n",
    "    yhat = clf.predict(Xte)\n",
    "    acc = accuracy_score(yte, yhat)\n",
    "    auc = roc_auc_score(yte, clf.predict_proba(Xte)[:,1])\n",
    "    return 100.0*acc, float(auc)\n",
    "\n",
    "def detection_score_bestof(real_std, synth_std, seed=123):\n",
    "    n = min(len(real_std), len(synth_std))\n",
    "    X = np.vstack([real_std[:n], synth_std[:n]])\n",
    "    y = np.hstack([np.zeros(n, dtype=int), np.ones(n, dtype=int)])\n",
    "    models = {\n",
    "        \"LogReg(L2)\": LogisticRegression(max_iter=5000, solver=\"lbfgs\"),\n",
    "        \"SVM-RBF\":    SVC(kernel=\"rbf\", C=10.0, gamma=\"scale\", probability=True, random_state=seed),\n",
    "        \"RF-400\":     RandomForestClassifier(n_estimators=400, n_jobs=-1, random_state=seed),\n",
    "    }\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    best_acc, best_auc, best_name = -1, -1, None\n",
    "    for name, clf in models.items():\n",
    "        accs, aucs = [], []\n",
    "        for tr, te in skf.split(X, y):\n",
    "            clf.fit(X[tr], y[tr])\n",
    "            p = clf.predict_proba(X[te])[:, 1]\n",
    "            accs.append(accuracy_score(y[te], (p >= 0.5).astype(int)))\n",
    "            aucs.append(roc_auc_score(y[te], p))\n",
    "        acc, auc = 100*np.mean(accs), float(np.mean(aucs))\n",
    "        if acc > best_acc:\n",
    "            best_acc, best_auc, best_name = acc, auc, name\n",
    "    return best_acc, best_auc, best_name\n",
    "\n",
    "# ---------------------------\n",
    "# Plot helpers\n",
    "# ---------------------------\n",
    "def save_corr_heatmaps_with_labels(real_std, synth_std, feat_names, tag):\n",
    "    C_real  = np.corrcoef(real_std, rowvar=False)\n",
    "    C_synth = np.corrcoef(synth_std, rowvar=False)\n",
    "    C_diff  = C_synth - C_real\n",
    "    fig, axs = plt.subplots(1,3, figsize=(18,6))\n",
    "    ims = [\n",
    "        axs[0].imshow(C_real,  vmin=-1, vmax=1),\n",
    "        axs[1].imshow(C_synth, vmin=-1, vmax=1),\n",
    "        axs[2].imshow(C_diff,  vmin=-0.5, vmax=0.5),\n",
    "    ]\n",
    "    for ax, title in zip(axs, [\"Corr: Real\", \"Corr: Synthetic\", \"Corr diff (S-R)\"]):\n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks(range(len(feat_names))); ax.set_yticks(range(len(feat_names)))\n",
    "        ax.set_xticklabels(feat_names, rotation=90, fontsize=7)\n",
    "        ax.set_yticklabels(feat_names, fontsize=7)\n",
    "    for im, ax in zip(ims, axs):\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.02)\n",
    "    plt.suptitle(f\"Correlation matrices — {tag}\")\n",
    "    plt.tight_layout()\n",
    "    fname = f\"{OUTDIR}/corr_heatmaps_{tag.lower()}_labeled.png\"\n",
    "    plt.savefig(fname, dpi=220); plt.close()\n",
    "    return fname\n",
    "\n",
    "def save_table_as_image(df, title, fname):\n",
    "    fig, ax = plt.subplots(figsize=(8.0, 1.6 + 0.35 * len(df)))\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=12, pad=10)\n",
    "    tbl = ax.table(cellText=df.values, colLabels=df.columns, loc='center', cellLoc='center')\n",
    "    tbl.auto_set_font_size(False); tbl.set_fontsize(10); tbl.scale(1.0, 1.25)\n",
    "    os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=220); plt.close()\n",
    "\n",
    "def save_umap_paperstyle(real_std, synth_std, tag, panel_label=\"(b) AIDS\", xlim=(-10,15), ylim=(-5,15)):\n",
    "    reducer = UMAP(\n",
    "        n_neighbors=5, min_dist=0.15, metric=\"euclidean\",\n",
    "        init=\"spectral\", random_state=SEED, transform_seed=SEED\n",
    "    )\n",
    "    R2 = reducer.fit_transform(real_std)    # fit on REAL only\n",
    "    S2 = reducer.transform(synth_std)       # transform SYNTH\n",
    "\n",
    "    def _affine_to_bounds(col, new_min, new_max):\n",
    "        cmin, cmax = float(col.min()), float(col.max())\n",
    "        scale = (new_max - new_min) / (cmax - cmin + 1e-12)\n",
    "        shift = new_min - cmin * scale\n",
    "        return col * scale + shift\n",
    "\n",
    "    R2s = np.column_stack([\n",
    "        _affine_to_bounds(R2[:,0], xlim[0], xlim[1]),\n",
    "        _affine_to_bounds(R2[:,1], ylim[0], ylim[1])\n",
    "    ])\n",
    "    S2s = np.column_stack([\n",
    "        _affine_to_bounds(S2[:,0], xlim[0], xlim[1]),\n",
    "        _affine_to_bounds(S2[:,1], ylim[0], ylim[1])\n",
    "    ])\n",
    "\n",
    "    fig = plt.figure(figsize=(6.2,5.0))\n",
    "    plt.scatter(R2s[:,0], R2s[:,1], s=10, marker=\".\", alpha=0.9, label=\"Real\")\n",
    "    plt.scatter(S2s[:,0], S2s[:,1], s=10, marker=\".\", alpha=0.9, label=\"Synthetic (TabGraphSyn)\")\n",
    "    plt.legend(loc=\"lower left\", frameon=True)\n",
    "    plt.xlim(*xlim); plt.ylim(*ylim)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.linspace(xlim[0], xlim[1], 6))\n",
    "    fig.subplots_adjust(bottom=0.18)\n",
    "    fig.text(0.5, 0.06, panel_label, ha=\"center\", va=\"center\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    fname = f\"{OUTDIR}/umap_{tag.lower()}_paperstyle.png\"\n",
    "    plt.savefig(fname, dpi=200); plt.close()\n",
    "    return fname\n",
    "\n",
    "# ---------------------------\n",
    "# Column aliasing & UCI loader\n",
    "# ---------------------------\n",
    "DROP_ID_LIKE = {\"pidnum\"}\n",
    "ALIASES = {\n",
    "    \"time\":  [\"time\",\"days\",\"t\",\"futime\",\"survt\"],\n",
    "    \"event\": [\"cid\",\"cens\",\"event\",\"status\",\"death\",\"fail\",\"delta\",\"died\"],\n",
    "    \"treat\": [\"trt\",\"treat\",\"arm\",\"rx\"],\n",
    "}\n",
    "\n",
    "def _clean_key(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    return \"\".join(ch for ch in s if ch.isalnum() or ch == \"_\")\n",
    "\n",
    "def _pick_first_present(cols, candidates):\n",
    "    for n in candidates:\n",
    "        if n in cols: return n\n",
    "    simp_map = {_clean_key(c): c for c in cols}\n",
    "    for n in candidates:\n",
    "        k = _clean_key(n)\n",
    "        if k in simp_map: return simp_map[k]\n",
    "    return None\n",
    "\n",
    "def _to_numeric_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in out.columns:\n",
    "        if pd.api.types.is_numeric_dtype(out[c]):\n",
    "            continue\n",
    "        try:\n",
    "            out[c] = out[c].astype(\"category\").cat.codes\n",
    "        except Exception:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "def load_actg175_from_ucirepo(uci_id: int = 890):\n",
    "    from ucimlrepo import fetch_ucirepo\n",
    "    ds = fetch_ucirepo(id=uci_id)\n",
    "\n",
    "    try:\n",
    "        print(\"=== UCI metadata (short) ===\")\n",
    "        print({\"name\": ds.metadata.get(\"name\")})\n",
    "        print(\"=== Variables (head) ===\")\n",
    "        if hasattr(ds, \"variables\"):\n",
    "            print(ds.variables.head())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    Xdf = ds.data.features.copy()\n",
    "    Ydf = ds.data.targets.copy() if hasattr(ds.data, \"targets\") and ds.data.targets is not None else pd.DataFrame()\n",
    "    raw = pd.concat([Xdf.reset_index(drop=True), Ydf.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    raw = raw[[c for c in raw.columns if c not in DROP_ID_LIKE]]\n",
    "    raw = raw.loc[:, ~raw.columns.duplicated()]\n",
    "    raw = _to_numeric_df(raw)\n",
    "\n",
    "    cols = list(raw.columns)\n",
    "    time_src  = _pick_first_present(cols, ALIASES[\"time\"])\n",
    "    event_src = _pick_first_present(cols, ALIASES[\"event\"])\n",
    "    treat_src = _pick_first_present(cols, ALIASES[\"treat\"])\n",
    "    if time_src is None:  raise KeyError(f\"No time column found in {cols} (tried {ALIASES['time']})\")\n",
    "    if event_src is None: raise KeyError(f\"No event column found in {cols} (tried {ALIASES['event']})\")\n",
    "    if treat_src is None: raise KeyError(f\"No treat column found in {cols} (tried {ALIASES['treat']})\")\n",
    "\n",
    "    df = raw.copy()\n",
    "    df[\"time\"]  = pd.to_numeric(df[time_src],  errors=\"coerce\")\n",
    "    df[\"event\"] = pd.to_numeric(df[event_src], errors=\"coerce\")\n",
    "    df[\"treat\"] = pd.to_numeric(df[treat_src], errors=\"coerce\")\n",
    "\n",
    "    to_drop = set(ALIASES[\"time\"] + ALIASES[\"event\"] + ALIASES[\"treat\"])\n",
    "    to_drop.discard(\"time\"); to_drop.discard(\"event\"); to_drop.discard(\"treat\")\n",
    "    to_drop = [c for c in to_drop if c in df.columns]\n",
    "    df = df.drop(columns=to_drop, errors=\"ignore\")\n",
    "\n",
    "    df = df.dropna(subset=[\"time\",\"event\"]).copy()\n",
    "    med_t = float(np.nanmedian(df[\"time\"].values))\n",
    "    df.loc[df[\"time\"] <= 0, \"time\"] = med_t if med_t > 0 else 1.0\n",
    "    df[\"event\"] = (df[\"event\"] > 0).astype(int)\n",
    "    df[\"treat\"] = (df[\"treat\"] > 0).astype(int)\n",
    "\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "    preferred_order = [\n",
    "        \"age\",\"wtkg\",\"hemo\",\"homo\",\"drugs\",\"karnof\",\"oprior\",\"z30\",\"zprior\",\"preanti\",\n",
    "        \"race\",\"gender\",\"str2\",\"strat\",\"symptom\",\"offtrt\",\"cd40\",\"cd420\",\"cd80\",\"cd820\",\n",
    "        \"time\",\"treat\",\"event\"\n",
    "    ]\n",
    "    ordered = [c for c in preferred_order if c in df.columns]\n",
    "    remaining = [c for c in df.columns if c not in ordered]\n",
    "    df = df[ordered + remaining]\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    feat_cols = [c for c in df.columns if c != \"event\"]  # keep time & treat as features\n",
    "    X = df[feat_cols].copy()\n",
    "    T = df[\"time\"].astype(float).values\n",
    "    E = df[\"event\"].astype(int).values\n",
    "    feat_names = list(feat_cols)\n",
    "\n",
    "    binaries = []\n",
    "    for c in df.columns:\n",
    "        arr = df[c].to_numpy()\n",
    "        if arr.ndim > 1: arr = arr.ravel()\n",
    "        uniq = np.unique(arr[~np.isnan(arr)]) if issubclass(arr.dtype.type, np.floating) else np.unique(arr)\n",
    "        if set(uniq.tolist()).issubset({0,1}):\n",
    "            binaries.append(c)\n",
    "    if \"treat\" not in binaries: binaries.append(\"treat\")\n",
    "    if \"event\" not in binaries: binaries.append(\"event\")\n",
    "\n",
    "    return df, X, T, E, feat_names, binaries, {\"time\":\"time\",\"event\":\"event\",\"treat\":\"treat\"}, \"AIDS\"\n",
    "\n",
    "# ---------------------------\n",
    "# Survival helpers + calibration (shrinkage + holdout)\n",
    "# ---------------------------\n",
    "def build_km_by_treatment(df, time_col, event_col, treat_col):\n",
    "    out = {}\n",
    "    xmax = float(np.nanmax(df[time_col].values))\n",
    "    for g in sorted(df[treat_col].unique()):\n",
    "        m = (df[treat_col].astype(int) == int(g)).to_numpy()\n",
    "        if m.sum() == 0:\n",
    "            continue\n",
    "        sf = SurvfuncRight(df.loc[m, time_col].to_numpy(),\n",
    "                           df.loc[m, event_col].to_numpy())\n",
    "        t = np.asarray(sf.surv_times, dtype=float)\n",
    "        s = np.asarray(sf.surv_prob,  dtype=float)\n",
    "        t = np.r_[0.0, t]; s = np.r_[1.0, s]\n",
    "        keep = t <= xmax\n",
    "        t, s = t[keep], s[keep]\n",
    "        if len(t) and t[-1] < xmax:\n",
    "            t = np.r_[t, xmax]; s = np.r_[s, s[-1]]\n",
    "        out[int(g)] = (t, s)\n",
    "    return out, xmax\n",
    "\n",
    "def _quantile_map_to_ref(source_vals, ref_vals):\n",
    "    s = np.asarray(source_vals, float)\n",
    "    r = np.asarray(ref_vals, float)\n",
    "    if len(s) == 0:\n",
    "        return s.copy()\n",
    "    order = np.argsort(s)\n",
    "    q = (np.arange(len(s)) + 0.5) / len(s)\n",
    "    try:\n",
    "        tgt_sorted = np.quantile(r, q, method=\"linear\")\n",
    "    except TypeError:\n",
    "        tgt_sorted = np.quantile(r, q, interpolation=\"linear\")\n",
    "    out = np.empty_like(s)\n",
    "    out[order] = tgt_sorted\n",
    "    return out\n",
    "\n",
    "def survival_calibrate_times_events_shrinkage(\n",
    "    real_df, synth_df, time_col=\"time\", event_col=\"event\", treat_col=\"treat\",\n",
    "    lambda_rate=LAMBDA_RATE, alpha_time=ALPHA_TIME, holdout_frac=HOLDOUT_SURV, seed=SEED\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    S = synth_df.copy()\n",
    "    S[treat_col] = (S[treat_col] > 0).astype(int)\n",
    "    real_tmp = real_df.copy()\n",
    "    real_tmp[treat_col] = (real_tmp[treat_col] > 0).astype(int)\n",
    "\n",
    "    # Stratified holdout for calibration\n",
    "    idx = np.arange(len(real_tmp))\n",
    "    tr_idx, te_idx = [], []\n",
    "    for g in sorted(real_tmp[treat_col].unique()):\n",
    "        for e in [0,1]:\n",
    "            mask = (real_tmp[treat_col]==g) & (real_tmp[event_col]==e)\n",
    "            ids = idx[mask.to_numpy()]\n",
    "            rng.shuffle(ids)\n",
    "            k = int(round((1.0-holdout_frac)*len(ids)))\n",
    "            tr_idx.extend(ids[:k]); te_idx.extend(ids[k:])\n",
    "    calib = real_tmp.iloc[tr_idx].reset_index(drop=True)\n",
    "    xmax = float(np.nanmax(real_tmp[time_col].values))\n",
    "\n",
    "    for g in sorted(S[treat_col].unique()):\n",
    "        g = int(g)\n",
    "        m_s = (S[treat_col] == g).to_numpy()\n",
    "        if m_s.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        Rg_ev = calib[(calib[treat_col]==g) & (calib[event_col]==1)][time_col].values\n",
    "        Rg_ce = calib[(calib[treat_col]==g) & (calib[event_col]==0)][time_col].values\n",
    "        if len(Rg_ev)==0 and len(Rg_ce)==0:\n",
    "            continue\n",
    "\n",
    "        syn_rate = float(S.loc[m_s, event_col].mean()) if event_col in S.columns else 0.0\n",
    "        real_rate = float(calib.loc[calib[treat_col]==g, event_col].mean()) if (calib[treat_col]==g).any() else syn_rate\n",
    "        target_rate = (1.0 - lambda_rate) * syn_rate + lambda_rate * real_rate\n",
    "        n_s = int(m_s.sum())\n",
    "        n_ev_target = int(round(target_rate * n_s))\n",
    "        n_ev_target = max(0, min(n_ev_target, n_s))\n",
    "\n",
    "        idx_grp = np.where(m_s)[0]\n",
    "        ev_now  = idx_grp[S.loc[idx_grp, event_col].astype(int).to_numpy()==1] if event_col in S.columns else np.array([], dtype=int)\n",
    "        ce_now  = idx_grp[S.loc[idx_grp, event_col].astype(int).to_numpy()==0] if event_col in S.columns else idx_grp\n",
    "\n",
    "        # adjust counts toward target\n",
    "        if len(ev_now) > n_ev_target:\n",
    "            rng.shuffle(ev_now); flip = ev_now[:len(ev_now)-n_ev_target]\n",
    "            S.loc[flip, event_col] = 0\n",
    "            ce_now = np.concatenate([ce_now, flip])\n",
    "            ev_now = np.setdiff1d(ev_now, flip, assume_unique=False)\n",
    "        elif len(ev_now) < n_ev_target:\n",
    "            need = n_ev_target - len(ev_now)\n",
    "            rng.shuffle(ce_now)\n",
    "            take = ce_now[:need]\n",
    "            S.loc[take, event_col] = 1\n",
    "            ev_now = np.concatenate([ev_now, take])\n",
    "            ce_now = np.setdiff1d(ce_now, take, assume_unique=False)\n",
    "\n",
    "        # partial quantile mapping\n",
    "        if len(ev_now) and len(Rg_ev):\n",
    "            boot_ev = rng.choice(Rg_ev, size=len(ev_now), replace=True)\n",
    "            mapped = _quantile_map_to_ref(S.loc[ev_now, time_col].values, boot_ev)\n",
    "            S.loc[ev_now, time_col] = (1.0 - alpha_time) * S.loc[ev_now, time_col].values + alpha_time * mapped\n",
    "        if len(ce_now) and len(Rg_ce):\n",
    "            boot_ce = rng.choice(Rg_ce, size=len(ce_now), replace=True)\n",
    "            mapped = _quantile_map_to_ref(S.loc[ce_now, time_col].values, boot_ce)\n",
    "            S.loc[ce_now, time_col] = (1.0 - alpha_time) * S.loc[ce_now, time_col].values + alpha_time * mapped\n",
    "\n",
    "    S[time_col] = np.clip(S[time_col].astype(float).values, 0.0, xmax)\n",
    "    S[event_col] = S[event_col].astype(int)\n",
    "    # tiny jitter to break ties\n",
    "    jitter = (np.nanpercentile(real_tmp[time_col], 75) - np.nanpercentile(real_tmp[time_col], 25) + 1e-9) * 1e-6\n",
    "    S[time_col] = S[time_col].values + np.random.default_rng(seed).normal(0.0, jitter, size=len(S))\n",
    "    S[time_col] = np.clip(S[time_col], 0.0, xmax)\n",
    "    return S\n",
    "\n",
    "def _surv_at(times, surv, x):\n",
    "    if x <= 0: return 1.0\n",
    "    idx = np.searchsorted(times, x, side=\"right\") - 1\n",
    "    idx = max(idx, 0)\n",
    "    return float(surv[idx])\n",
    "\n",
    "def km_plot_by_treatment(real_df, synth_df, time_col, event_col, treat_col, tag):\n",
    "    from matplotlib.lines import Line2D\n",
    "    km_real, xmax = build_km_by_treatment(real_df, time_col, event_col, treat_col)\n",
    "    km_syn,  _    = build_km_by_treatment(\n",
    "        synth_df.assign(**{time_col: np.minimum(synth_df[time_col].values, xmax)}),\n",
    "        time_col, event_col, treat_col\n",
    "    )\n",
    "    colors = {0: \"#1f77b4\", 1: \"#2ca02c\"}  # blue, green\n",
    "\n",
    "    plt.figure(figsize=(7.8, 5.0))\n",
    "    for g in [0, 1]:\n",
    "        if g not in km_real or g not in km_syn:\n",
    "            continue\n",
    "        rt, rp = km_real[g]; st, sp = km_syn[g]\n",
    "        plt.step(rt, rp, where=\"post\", color=colors[g], linewidth=2.0)\n",
    "        plt.step(st, sp, where=\"post\", color=colors[g], linewidth=2.0, linestyle=\"--\")\n",
    "        for d, label in [(365, \"1 yr\"), (1095, \"3 yr\")]:\n",
    "            if d <= xmax:\n",
    "                r_val = _surv_at(rt, rp, d)\n",
    "                s_val = _surv_at(st, sp, d)\n",
    "                voff = 0.03 if g == 1 else -0.06\n",
    "                plt.text(d + 10, r_val + voff, f\"{label} (R {r_val:.2f}, V {s_val:.2f})\",\n",
    "                         color=colors[g], fontsize=9)\n",
    "\n",
    "    for d in [365, 1095]:\n",
    "        if d <= xmax: plt.axvline(d, linestyle=\":\", color=\"red\", alpha=0.35)\n",
    "\n",
    "    plt.xlim(0, xmax * 1.02); plt.ylim(0.6, 1.0)\n",
    "    plt.xlabel(\"Time (days)\"); plt.ylabel(\"Survival Probability\")\n",
    "    plt.title(\"Kaplan–Meier: Real vs Synthetic by treatment\")\n",
    "\n",
    "    treatment_handles = [\n",
    "        Line2D([0], [0], color=colors[0], lw=3, label=\"Treat 0\"),\n",
    "        Line2D([0], [0], color=colors[1], lw=3, label=\"Treat 1\"),\n",
    "    ]\n",
    "    style_handles = [\n",
    "        Line2D([0], [0], color=\"k\", lw=2, linestyle=\"-\",  label=\"Real\"),\n",
    "        Line2D([0], [0], color=\"k\", lw=2, linestyle=\"--\", label=\"Synthetic (TabGraphSyn)\"),\n",
    "    ]\n",
    "    leg1 = plt.legend(handles=treatment_handles, title=\"Treatment\", loc=\"lower left\")\n",
    "    plt.gca().add_artist(leg1)\n",
    "    plt.legend(handles=style_handles, title=\"Data Source\", loc=\"lower right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fname = f\"{OUTDIR}/survival_km_{tag.lower()}.png\"\n",
    "    plt.savefig(fname, dpi=220); plt.close()\n",
    "    return fname\n",
    "\n",
    "# ---------------------------\n",
    "# Diffusion helpers\n",
    "# ---------------------------\n",
    "def cosine_beta_schedule(T, s=0.008):\n",
    "    steps = T + 1\n",
    "    x = torch.linspace(0, T, steps, device=device)\n",
    "    alphas_cumprod = torch.cos(((x / T) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 1e-5, 0.999)\n",
    "\n",
    "# ---------------------------\n",
    "# VAE extra losses for marginals\n",
    "# ---------------------------\n",
    "def batch_mde_loss_1d(x_rec, x_true):\n",
    "    \"\"\"\n",
    "    Approximate marginal KS/W1 with batch-quantile L1:\n",
    "    Sort per feature along batch, then mean |Δ| across features.\n",
    "    \"\"\"\n",
    "    # x_* shape: [B, D]\n",
    "    x1, _ = torch.sort(x_rec, dim=0)\n",
    "    x2, _ = torch.sort(x_true, dim=0)\n",
    "    return F.l1_loss(x1, x2, reduction='mean')\n",
    "\n",
    "def batch_moment_loss(x_rec, x_true, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Mean/std alignment across batch, averaged over features.\n",
    "    \"\"\"\n",
    "    m1 = x_rec.mean(dim=0)\n",
    "    s1 = x_rec.std(dim=0) + eps\n",
    "    m2 = x_true.mean(dim=0)\n",
    "    s2 = x_true.std(dim=0) + eps\n",
    "    return F.mse_loss(m1, m2) + F.mse_loss(s1, s2)\n",
    "\n",
    "# ---------------------------\n",
    "# Marginal calibration (post-decode)\n",
    "# ---------------------------\n",
    "def marginal_calibrate_blend(real_df, synth_df, binary_cols, time_col, event_col, treat_col,\n",
    "                             alpha_marg=ALPHA_MARG, holdout_frac=HOLDOUT_MARG, bin_shrink=BIN_SHRINK, seed=SEED):\n",
    "    \"\"\"\n",
    "    Holdout-aware, feature-wise calibration:\n",
    "      - For non-binary (excluding time/event): partial rank-preserving quantile map.\n",
    "      - For binary (excluding event): top-k rounding with shrinkage to real prevalence.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    real = real_df.copy()\n",
    "    # Split calibration / holdout (stratify by treat to avoid drift)\n",
    "    idx = np.arange(len(real))\n",
    "    tr_idx, _ = train_test_split(idx, test_size=holdout_frac, random_state=SEED,\n",
    "                                 stratify=real[treat_col] if treat_col in real.columns else None)\n",
    "    calib = real.iloc[tr_idx].reset_index(drop=True)\n",
    "\n",
    "    S = synth_df.copy()\n",
    "    # ----- Binary columns -----\n",
    "    bin_cols = [c for c in binary_cols if c in S.columns and c not in {event_col}]\n",
    "    for c in bin_cols:\n",
    "        # Don't remap 'treat' here; its distribution is enforced by generation\n",
    "        if c == treat_col:\n",
    "            # still snap to {0,1} using rank-based top-k at current prevalence\n",
    "            vals = S[c].to_numpy()\n",
    "            order = np.argsort(vals)\n",
    "            k = int(round(np.clip(np.mean(S[c]>0.5)*len(S), 0, len(S))))\n",
    "            S.loc[order[:len(S)-k], c] = 0\n",
    "            S.loc[order[len(S)-k:], c] = 1\n",
    "            continue\n",
    "        p_real = float(np.mean(calib[c].astype(int)))\n",
    "        p_curr = float(np.mean((S[c].values > 0.5).astype(int)))\n",
    "        p_tgt  = (1.0 - bin_shrink) * p_curr + bin_shrink * p_real\n",
    "        k = int(round(np.clip(p_tgt * len(S), 0, len(S))))\n",
    "        vals = S[c].to_numpy()\n",
    "        order = np.argsort(vals)  # largest become 1\n",
    "        S.loc[order[:len(S)-k], c] = 0\n",
    "        S.loc[order[len(S)-k:], c] = 1\n",
    "        S[c] = S[c].astype(int)\n",
    "\n",
    "    # ----- Non-binary columns (exclude time & event) -----\n",
    "    nb_cols = [c for c in S.columns if c not in set(bin_cols) | {time_col, event_col}]\n",
    "    for c in nb_cols:\n",
    "        r = calib[c].values\n",
    "        s = S[c].values\n",
    "        mapped = _quantile_map_to_ref(s, r)\n",
    "        S[c] = (1.0 - alpha_marg) * s + alpha_marg * mapped\n",
    "\n",
    "    return S\n",
    "\n",
    "# ---------------------------\n",
    "# Main pipeline\n",
    "# ---------------------------\n",
    "def run_pipeline_from_df(full_df, time_col=\"time\", event_col=\"event\", treat_col=\"treat\",\n",
    "                         tag=\"AIDS\", binary_cols=None):\n",
    "    # 1) Prepare matrices\n",
    "    feat_cols = [c for c in full_df.columns if c != event_col]\n",
    "    X = full_df[feat_cols].copy()\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X.values.astype(np.float32)).astype(np.float32)\n",
    "    feat_names = list(feat_cols)\n",
    "\n",
    "    # Conditional label = treatment (fallback to KMeans if constant)\n",
    "    tcol = full_df[treat_col]\n",
    "    if isinstance(tcol, pd.DataFrame): tcol = tcol.iloc[:,0]\n",
    "    treat = tcol.values.astype(int) if treat_col in full_df.columns else np.zeros(len(full_df), dtype=int)\n",
    "\n",
    "    X_train, X_val, treat_train, treat_val = train_test_split(\n",
    "        Xs, treat, test_size=0.2,\n",
    "        stratify=treat if len(np.unique(treat))>1 else None,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    X_all_t   = torch.tensor(Xs, device=device)\n",
    "    n_nodes, n_features = Xs.shape\n",
    "\n",
    "    # 2) kNN graph\n",
    "    nbrs = NearestNeighbors(n_neighbors=KNN_K+1).fit(Xs)\n",
    "    _, idxs = nbrs.kneighbors(Xs)\n",
    "    A = np.zeros((n_nodes, n_nodes), dtype=np.float32)\n",
    "    for i in range(n_nodes):\n",
    "        for j in idxs[i][1:]:\n",
    "            A[i, j] = 1.0; A[j, i] = 1.0\n",
    "    I = np.eye(n_nodes, dtype=np.float32)\n",
    "    A_hat = A + I\n",
    "    D = np.sum(A_hat, axis=1)\n",
    "    D_inv_sqrt = 1.0 / np.sqrt(D + 1e-8)\n",
    "    A_norm = D_inv_sqrt[:, None] * A_hat * D_inv_sqrt[None, :]\n",
    "    A_norm_t = torch.tensor(A_norm, device=device)\n",
    "\n",
    "    # 3) GCN conditioning\n",
    "    y = treat.copy()\n",
    "    if len(np.unique(y)) < 2:\n",
    "        from sklearn.cluster import KMeans\n",
    "        y = KMeans(n_clusters=2, random_state=SEED).fit_predict(Xs)\n",
    "    y_all_t = torch.tensor(y, device=device)\n",
    "\n",
    "    idx_all = np.arange(n_nodes)\n",
    "    train_idx, val_idx = train_test_split(idx_all, test_size=0.2, stratify=y, random_state=SEED)\n",
    "    train_mask = np.zeros(n_nodes, dtype=bool); train_mask[train_idx] = True\n",
    "    val_mask   = np.zeros(n_nodes, dtype=bool); val_mask[val_idx]   = True\n",
    "    train_mask_t = torch.tensor(train_mask, device=device)\n",
    "    val_mask_t   = torch.tensor(val_mask,   device=device)\n",
    "\n",
    "    gcn = GCN(n_features).to(device)\n",
    "    opt_gcn = torch.optim.AdamW(gcn.parameters(), lr=GCN_LR, weight_decay=GCN_WD)\n",
    "    for ep in range(GCN_EPOCHS):\n",
    "        gcn.train(); opt_gcn.zero_grad()\n",
    "        logits, _ = gcn(A_norm_t, X_all_t)\n",
    "        loss = F.cross_entropy(logits[train_mask_t], y_all_t[train_mask_t])\n",
    "        loss.backward(); torch.nn.utils.clip_grad_norm_(gcn.parameters(), 1.0)\n",
    "        opt_gcn.step()\n",
    "    with torch.no_grad():\n",
    "        gcn.eval(); _, cond_all = gcn(A_norm_t, X_all_t)\n",
    "    cond_dim = cond_all.shape[1]\n",
    "\n",
    "    # class prototypes\n",
    "    cond_class = []\n",
    "    classes = np.unique(y)\n",
    "    for c in classes:\n",
    "        mask = (y_all_t == int(c))\n",
    "        cond_class.append(cond_all[mask].mean(dim=0, keepdim=True))\n",
    "    cond_class = torch.cat(cond_class, dim=0)\n",
    "\n",
    "    # 4) VAE with KL warm-up + MDE/Moment losses\n",
    "    def vae_step_losses(rec, xb, mu, lv, beta):\n",
    "        _, _, _, _, rl_dummy, kld_dummy = 0,0,0,0,0,0  # just for naming clarity\n",
    "        # main recon/KL are computed in forward; recompute here for clarity\n",
    "        rl = F.mse_loss(rec, xb, reduction='mean')\n",
    "        kld = -0.5 * torch.mean(1 + lv - mu.pow(2) - lv.exp())\n",
    "        mde = batch_mde_loss_1d(rec, xb)\n",
    "        mom = batch_moment_loss(rec, xb)\n",
    "        return rl + beta*kld + MDE_W*mde + MOMENT_W*mom, rl.item(), kld.item(), mde.item(), mom.item()\n",
    "\n",
    "    vae = VAE(in_dim=n_features, z_dim=VAE_Z, hidden=VAE_H).to(device)\n",
    "    opt_vae = torch.optim.AdamW(vae.parameters(), lr=VAE_LR, weight_decay=VAE_WD)\n",
    "    X_train_t = torch.tensor(X_train, device=device)\n",
    "    bs = VAE_BS\n",
    "    for ep in range(VAE_EPOCHS):\n",
    "        vae.train()\n",
    "        beta = BETA_MAX * min(1.0, ep / max(1, int(0.4*VAE_EPOCHS)))  # warm-up over 40% epochs\n",
    "        idx = torch.randperm(X_train_t.shape[0], device=device)\n",
    "        for i in range(0, len(idx), bs):\n",
    "            xb = X_train_t[idx[i:i+bs]]\n",
    "            opt_vae.zero_grad(); rec, mu, lv, z, _, _ = vae(xb, beta=beta)\n",
    "            loss, rl_v, kld_v, mde_v, mom_v = vae_step_losses(rec, xb, mu, lv, beta)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vae.parameters(), 1.0)\n",
    "            opt_vae.step()\n",
    "    for p in vae.parameters(): p.requires_grad = False\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        mu_all, lv_all = vae.encode(torch.tensor(Xs, device=device)); z_all = mu_all\n",
    "    z_train = z_all[train_mask_t]\n",
    "\n",
    "    # 5) DDPM\n",
    "    betas = cosine_beta_schedule(TSTEPS)\n",
    "    alphas = 1.0 - betas\n",
    "    acp = torch.cumprod(alphas, 0)\n",
    "    sqrt_acp    = torch.sqrt(acp)\n",
    "    sqrt_1m_acp = torch.sqrt(1.0 - acp)\n",
    "    def q_sample(x0, t, noise):\n",
    "        return sqrt_acp[t].unsqueeze(1)*x0 + sqrt_1m_acp[t].unsqueeze(1)*noise\n",
    "\n",
    "    eps_net = EpsNet(z_dim=z_all.shape[1], c_dim=cond_dim).to(device)\n",
    "    opt_eps = torch.optim.AdamW(eps_net.parameters(), lr=EPS_LR, weight_decay=EPS_WD)\n",
    "    ema_net = EpsNet(z_dim=z_all.shape[1], c_dim=cond_dim).to(device)\n",
    "    ema_net.load_state_dict(eps_net.state_dict())\n",
    "    def update_ema(student, teacher, decay=0.9995):\n",
    "        for p_s, p_t in zip(student.parameters(), teacher.parameters()):\n",
    "            p_t.data.mul_(decay).add_(p_s.data, alpha=1.0 - decay)\n",
    "\n",
    "    for ep in range(DDPM_EPOCHS):\n",
    "        eps_net.train(); perm = torch.randperm(z_train.shape[0], device=device)\n",
    "        for i in range(0, len(perm), 128):\n",
    "            idx = perm[i:i+128]\n",
    "            x0 = z_train[idx]\n",
    "            c  = cond_all[train_mask_t][idx] + COND_NOISE * torch.randn_like(cond_all[train_mask_t][idx])\n",
    "            t  = torch.randint(low=0, high=TSTEPS, size=(x0.shape[0],), device=device, dtype=torch.long)\n",
    "            noise = torch.randn_like(x0)\n",
    "            x_t = q_sample(x0, t, noise)\n",
    "            drop = (torch.rand(x0.size(0), device=device) < DROP_UNCOND).float().unsqueeze(1)\n",
    "            c_step = c * (1.0 - drop)\n",
    "            opt_eps.zero_grad(); pred = eps_net(x_t, c_step, t, TSTEPS)\n",
    "            loss = F.mse_loss(pred, noise); loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(eps_net.parameters(), 1.0)\n",
    "            opt_eps.step(); update_ema(eps_net, ema_net)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_mean_var_cfg(x_t, t, c, w=CFG_W):\n",
    "        beta_t = betas[t].unsqueeze(1)\n",
    "        sqrt_recip_alpha = (1.0/torch.sqrt(alphas[t])).unsqueeze(1)\n",
    "        sqrt_1m = sqrt_1m_acp[t].unsqueeze(1)\n",
    "        net = ema_net\n",
    "        eps_c = net(x_t, c, t, TSTEPS)\n",
    "        eps_u = net(x_t, torch.zeros_like(c), t, TSTEPS)\n",
    "        eps_hat = (1.0 + w) * eps_c - w * eps_u\n",
    "        mean = sqrt_recip_alpha * (x_t - (beta_t / sqrt_1m) * eps_hat)\n",
    "        var  = betas[t].unsqueeze(1)\n",
    "        return mean, var\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_latents_cond(n_per_class, w=CFG_W, tau=TAU_NOISE):\n",
    "        xs=[]; ys=[]\n",
    "        for cls, n_ in enumerate(n_per_class):\n",
    "            if n_ <= 0: continue\n",
    "            x = torch.randn(n_, z_all.shape[1], device=device)\n",
    "            c = cond_class[cls].expand(n_, -1)\n",
    "            for t in reversed(range(TSTEPS)):\n",
    "                t_b = torch.full((n_,), t, device=device, dtype=torch.long)\n",
    "                mean, var = p_mean_var_cfg(x, t_b, c, w=w)\n",
    "                if t>0: x = mean + torch.sqrt(var) * tau * torch.randn_like(x)\n",
    "                else:   x = mean\n",
    "            xs.append(x); ys.append(np.full(n_, cls, dtype=int))\n",
    "        return (torch.cat(xs, 0) if xs else torch.empty(0, z_all.shape[1], device=device),\n",
    "                np.concatenate(ys) if ys else np.array([], dtype=int))\n",
    "\n",
    "    # Generate ~3x synthetic, class-matched by treatment proportions\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    prop = counts / counts.sum()\n",
    "    n_samples = len(y) * 3\n",
    "    n_per_class = (prop * n_samples).astype(int)\n",
    "    while n_per_class.sum() < n_samples:\n",
    "        n_per_class[np.argmax(prop)] += 1\n",
    "\n",
    "    z_synth, y_synth = sample_latents_cond(n_per_class)\n",
    "    with torch.no_grad():\n",
    "        x_synth_std0 = vae.decode(z_synth).cpu().numpy().astype(np.float32)\n",
    "\n",
    "    # CORAL (standardized space) + blend + jitter\n",
    "    cov_reg = CORAL_REG\n",
    "    def sym_eig(mat, eps=1e-8):\n",
    "        w, v = np.linalg.eigh((mat + mat.T) * 0.5)\n",
    "        w = np.clip(w, eps, None)\n",
    "        return w, v\n",
    "    mu_r = Xs.mean(0); Cr = np.cov(Xs, rowvar=False) + cov_reg*np.eye(n_features)\n",
    "    mu_s = x_synth_std0.mean(0); Cs = np.cov(x_synth_std0, rowvar=False) + cov_reg*np.eye(n_features)\n",
    "    ws, Vs = sym_eig(Cs); wt, Vt = sym_eig(Cr)\n",
    "    Cs_inv_sqrt = Vs @ np.diag(1.0/np.sqrt(ws)) @ Vs.T\n",
    "    Cr_sqrt     = Vt @ np.diag(np.sqrt(wt))     @ Vt.T\n",
    "    Xs0 = x_synth_std0 - mu_s\n",
    "    Xs_whiten  = Xs0 @ Cs_inv_sqrt\n",
    "    Xs_coral   = Xs_whiten @ Cr_sqrt + mu_r\n",
    "    Xs_blended = (1.0 - CORAL_BLEND) * x_synth_std0 + CORAL_BLEND * Xs_coral\n",
    "    Xs_blended += JITTER_SD * np.random.randn(*Xs_blended.shape).astype(Xs_blended.dtype)\n",
    "    synth_std_stage = Xs_blended.astype(np.float32)\n",
    "\n",
    "    # Inverse-scale → original feature space\n",
    "    synth_inv = scaler.inverse_transform(synth_std_stage)\n",
    "    synth_df = pd.DataFrame(synth_inv, columns=feat_names)\n",
    "\n",
    "    # Ensure 'treat' follows generated class, snap to {0,1}; placeholder for event\n",
    "    if treat_col in synth_df.columns:\n",
    "        synth_df[treat_col] = (y_synth[:len(synth_df)] if len(y_synth)>=len(synth_df)\n",
    "                               else np.rint(synth_df[treat_col]).astype(int)).astype(int)\n",
    "    if \"event\" not in synth_df.columns:\n",
    "        synth_df[\"event\"] = 0\n",
    "\n",
    "    # ----- NEW: Marginal calibration (binary + non-binary; excludes time/event) -----\n",
    "    real_for_marg = full_df.copy()\n",
    "    synth_df = marginal_calibrate_blend(\n",
    "        real_for_marg, synth_df, binary_cols=(binary_cols or []),\n",
    "        time_col=time_col, event_col=event_col, treat_col=treat_col,\n",
    "        alpha_marg=ALPHA_MARG, holdout_frac=HOLDOUT_MARG, bin_shrink=BIN_SHRINK, seed=SEED\n",
    "    )\n",
    "\n",
    "    # --- Survival calibration (times + event) ---\n",
    "    real_surv_df = full_df[[time_col, event_col, treat_col]].copy()\n",
    "    synth_df = survival_calibrate_times_events_shrinkage(\n",
    "        real_surv_df, synth_df,\n",
    "        time_col=time_col, event_col=event_col, treat_col=treat_col,\n",
    "        lambda_rate=LAMBDA_RATE, alpha_time=ALPHA_TIME, holdout_frac=HOLDOUT_SURV, seed=SEED\n",
    "    )\n",
    "\n",
    "    # Ensure positive times\n",
    "    synth_df.loc[synth_df[time_col] <= 0, time_col] = max(1.0, float(np.median(synth_df[time_col])))\n",
    "\n",
    "    # Save calibrated synthetic (original scale)\n",
    "    csv_out = f\"{OUTDIR}/synthetic_aids_with_survival.csv\"\n",
    "    synth_df.to_csv(csv_out, index=False)\n",
    "\n",
    "    # ----- IMPORTANT: Recompute standardized synthetic AFTER all calibrations -----\n",
    "    synth_std = scaler.transform(synth_df[feat_cols].values.astype(np.float32)).astype(np.float32)\n",
    "\n",
    "    # 6.1) Fidelity metrics\n",
    "    MDE_per_var_df, MDE_mean_pct = marginal_KS_percent(Xs, synth_std, feat_names)\n",
    "    P_corr_df, P_corr_mean_pct   = pairwise_corr_error_percent(Xs, synth_std, feat_names)\n",
    "    MDE_per_var_df.to_csv(f\"{OUTDIR}/marginal_errors_per_variable_aids.csv\", index=False)\n",
    "    P_corr_df.to_csv(f\"{OUTDIR}/pairwise_corr_errors_aids.csv\", index=False)\n",
    "    table2 = pd.DataFrame([[\"TabGraphSyn (marginal-aware)\", round(MDE_mean_pct, 2), round(P_corr_mean_pct, 2)]],\n",
    "                          columns=[\"Method\", \"Marginal Distribution Errors (%)\", \"Pairwise Correlation Errors (%)\"])\n",
    "    save_table_as_image(table2, \"TABLE II: Statistical Fidelity — AIDS\", f\"{OUTDIR}/table2_aids.png\")\n",
    "\n",
    "    # 6.2) UMAP\n",
    "    umap_path = save_umap_paperstyle(Xs, synth_std, tag=tag, panel_label=\"(b) AIDS\", xlim=(-10,15), ylim=(-5,15))\n",
    "\n",
    "    # 6.3) Correlation heatmaps\n",
    "    corr_path = save_corr_heatmaps_with_labels(Xs, synth_std, feat_names, tag=tag)\n",
    "\n",
    "    # 6.4) Survival analysis & Cox confusion (statsmodels)\n",
    "    real_df = pd.concat([X.reset_index(drop=True), full_df[[event_col]].reset_index(drop=True)], axis=1)\n",
    "    km_path = km_plot_by_treatment(real_df, synth_df, time_col=time_col, event_col=event_col, treat_col=treat_col, tag=tag)\n",
    "    cm, sig_real, sig_syn = cox_significance_confusion(real_df, synth_df, time_col=time_col, event_col=event_col)\n",
    "    save_table_as_image(cm, \"TABLE V: CoxPH significant covariates — Confusion Matrix (AIDS)\",\n",
    "                        f\"{OUTDIR}/table5_aids_cox_confusion.png\")\n",
    "\n",
    "    # 6.5) Detection score\n",
    "    det_log_acc, det_log_auc = detection_score_logistic(Xs, synth_std)\n",
    "    det_best_acc, det_best_auc, det_best_name = detection_score_bestof(Xs, synth_std, seed=SEED)\n",
    "    table6a = pd.DataFrame([[\"TabGraphSyn (LogReg)\", f\"{det_log_acc:.2f}%\", f\"{det_log_auc:.3f}\"]], columns=[\"Model\", \"Detection Acc (%)\", \"AUC\"])\n",
    "    save_table_as_image(table6a, \"TABLE VI-A: Detection (Logistic) — AIDS\", f\"{OUTDIR}/table6_aids_detection_logreg.png\")\n",
    "    table6b = pd.DataFrame([[f\"Best-of ({det_best_name})\", f\"{det_best_acc:.2f}%\", f\"{det_best_auc:.3f}\"]], columns=[\"Model\", \"Detection Acc (%)\", \"AUC\"])\n",
    "    save_table_as_image(table6b, \"TABLE VI-B: Detection (Best-of) — AIDS\", f\"{OUTDIR}/table6_aids_detection_bestof.png\")\n",
    "\n",
    "    # Per-variable marginal error table image\n",
    "    save_table_as_image(MDE_per_var_df.round({\"KS%\":2}),\n",
    "                        \"Marginal Errors per Variable — AIDS\",\n",
    "                        f\"{OUTDIR}/marginal_errors_per_variable_aids.png\")\n",
    "\n",
    "    # Console summary\n",
    "    print(\"\\n=== AIDS (ACTG175) — FINAL METRICS (marginal-aware) ===\")\n",
    "    print(pd.Series({\n",
    "        \"Marginal KS mean %\": MDE_mean_pct,\n",
    "        \"Pairwise |Δρ| mean %\": P_corr_mean_pct,\n",
    "        \"Detection Acc (%) — Logistic\": det_log_acc,\n",
    "        \"Detection AUC — Logistic\": det_log_auc,\n",
    "        \"Detection Acc (%) — Best-of\": det_best_acc,\n",
    "        \"Detection AUC — Best-of\": det_best_auc,\n",
    "        \"Best-of attacker\": det_best_name,\n",
    "    }))\n",
    "    print(\"Saved:\")\n",
    "    print(\" • Synthetic CSV:\", csv_out)\n",
    "    print(\" • UMAP:\", umap_path)\n",
    "    print(\" • Heatmaps:\", corr_path)\n",
    "    print(\" • Table II:\", f\"{OUTDIR}/table2_aids.png\")\n",
    "    print(\" • Marginal per-variable (CSV):\", f\"{OUTDIR}/marginal_errors_per_variable_aids.csv\")\n",
    "    print(\" • Pairwise errors (CSV):\", f\"{OUTDIR}/pairwise_corr_errors_aids.csv\")\n",
    "    print(\" • Survival KM:\", f\"{OUTDIR}/survival_km_{tag.lower()}.png\")\n",
    "    print(\" • Cox confusion:\", f\"{OUTDIR}/table5_aids_cox_confusion.png\")\n",
    "    print(\" • Detection Logistic:\", f\"{OUTDIR}/table6_aids_detection_logreg.png\")\n",
    "    print(\" • Detection Best-of:\", f\"{OUTDIR}/table6_aids_detection_bestof.png\")\n",
    "\n",
    "def cox_significance_confusion(real_df, synth_df, time_col, event_col):\n",
    "    def _fit(df):\n",
    "        exog = df.drop(columns=[time_col, event_col]).copy()\n",
    "        exog = sm.add_constant(exog, has_constant='add')\n",
    "        res = PHReg(df[time_col].values, exog.values, status=df[event_col].values).fit()\n",
    "        names = list(exog.columns)\n",
    "        pvals = np.asarray(res.pvalues).ravel()\n",
    "        summ = pd.DataFrame({\"feature\": names, \"p\": pvals})\n",
    "        sig = set(summ.loc[summ['p'] < 0.05, 'feature'].tolist())\n",
    "        sig.discard('const')\n",
    "        return sig, summ\n",
    "\n",
    "    sig_real, summ_real = _fit(real_df)\n",
    "    sig_syn,  summ_syn  = _fit(synth_df)\n",
    "    features = sorted(set(summ_real['feature']).union(set(summ_syn['feature'])))\n",
    "    tp = len(sig_real & sig_syn)\n",
    "    fp = len(sig_syn - sig_real)\n",
    "    fn = len(sig_real - sig_syn)\n",
    "    tn = len(set(features) - (sig_real | sig_syn))\n",
    "    prec = tp / (tp + fp + 1e-12)\n",
    "    rec  = tp / (tp + fn + 1e-12)\n",
    "    f1   = 2*prec*rec / (prec + rec + 1e-12)\n",
    "    cm = pd.DataFrame({\n",
    "        \"Metric\":[\"True Pos.\",\"False Pos.\",\"False Neg.\",\"True Neg.\",\"Precision\",\"Recall\",\"F1 Score\"],\n",
    "        \"TabGraphSyn\":[tp, fp, fn, tn, f\"{prec:.3f}\", f\"{rec:.3f}\", f\"{f1:.3f}\"]\n",
    "    })\n",
    "    return cm, sig_real, sig_syn\n",
    "\n",
    "def run_pipeline_from_ucirepo(uci_id: int = 890):\n",
    "    full_df, X, T, E, feat_names, bin_cols, colnames, tag = load_actg175_from_ucirepo(uci_id=uci_id)\n",
    "    return run_pipeline_from_df(\n",
    "        full_df,\n",
    "        time_col=colnames[\"time\"],\n",
    "        event_col=colnames[\"event\"],\n",
    "        treat_col=colnames[\"treat\"],\n",
    "        tag=tag,\n",
    "        binary_cols=bin_cols\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# CLI\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--uci_id\", type=int, default=890, help=\"UCI dataset id (default: 890 for ACTG175).\")\n",
    "    args, _unknown = parser.parse_known_args()\n",
    "    run_pipeline_from_ucirepo(args.uci_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22341624-fcab-4096-8f0f-0a21bcacab0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

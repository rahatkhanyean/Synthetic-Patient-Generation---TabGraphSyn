{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9edce8e-dad8-4258-b271-9dfec0a64ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: pandas in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (1.5.3)\n",
      "Requirement already satisfied: torch in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in /home/rkhan5/.local/lib/python3.11/site-packages (1.7.1)\n",
      "Requirement already satisfied: matplotlib in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (3.7.1)\n",
      "Requirement already satisfied: ucimlrepo in /home/rkhan5/.local/lib/python3.11/site-packages (0.0.7)\n",
      "Requirement already satisfied: networkx in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (3.1)\n",
      "Requirement already satisfied: umap-learn in /home/rkhan5/.local/lib/python3.11/site-packages (0.5.9.post2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: filelock in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/rkhan5/.local/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from ucimlrepo) (2023.7.22)\n",
      "Requirement already satisfied: numba>=0.51.2 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from umap-learn) (0.57.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/rkhan5/.local/lib/python3.11/site-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: tqdm in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from umap-learn) (4.65.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from numba>=0.51.2->umap-learn) (0.40.0)\n",
      "Requirement already satisfied: six>=1.5 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "\n",
    "!pip install numpy pandas torch scikit-learn matplotlib ucimlrepo networkx umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdcf1f47-c896-4d68-a685-cc6fabd4938b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rkhan5/.local/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WBCD: FINAL METRICS (after optimizations + CC marginal blend) ===\n",
      "Blend alpha (0=CORAL, 1=CC-QM)          1.0\n",
      "Marginal KS mean %                 0.322203\n",
      "Pairwise |Δρ| mean %               9.950451\n",
      "TSTR Accuracy                      0.964912\n",
      "TSTR F1                            0.972603\n",
      "TSTR AUC                           0.996032\n",
      "Detection Acc (%) — Logistic      73.684211\n",
      "Detection AUC — Logistic           0.770699\n",
      "Detection Acc (%) — Best-of       82.427158\n",
      "Detection AUC — Best-of            0.905876\n",
      "Best-of attacker                    SVM-RBF\n",
      "dtype: object\n",
      "Saved:\n",
      " • CSV: /data/user/home/rkhan5/Personal/outputs/synthetic_wbcd_with_labels.csv\n",
      " • UMAP: /data/user/home/rkhan5/Personal/outputs/umap_wbcd_paperstyle.png\n",
      " • Heatmaps: /data/user/home/rkhan5/Personal/outputs/corr_heatmaps_wbcd_labeled.png\n",
      " • Table II: /data/user/home/rkhan5/Personal/outputs/table2_wbcd.png\n",
      " • TSTR (Table IV): /data/user/home/rkhan5/Personal/outputs/table4_wbcd_tstr.png\n",
      " • Detection Logistic: /data/user/home/rkhan5/Personal/outputs/table6_wbcd_detection_logreg.png\n",
      " • Detection Best-of : /data/user/home/rkhan5/Personal/outputs/table6_wbcd_detection_bestof.png\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# TabGraphSyn (GCN → VAE → latent DDPM) — WBCD only (optimized)\n",
    "# Outputs:\n",
    "#   1) UMAP (UMAP-only; fixed axes & caption like paper)\n",
    "#   2) Marginal KS% & Pairwise |Δρ|% (Table II numbers)\n",
    "#   3) Correlation heatmaps (Real / Synthetic / Diff) with axis labels\n",
    "#   4) TSTR (train on synthetic, test on real): Accuracy, F1, AUC\n",
    "#   5) Detection Score (accuracy ↑): Logistic and Best-of (RF/SVM/LogReg)\n",
    "# ================================================================\n",
    "\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from umap import UMAP  # pip install umap-learn\n",
    "\n",
    "# ---------------------------\n",
    "# Reproducibility & device\n",
    "# ---------------------------\n",
    "SEED = 123\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---------------------------\n",
    "# Output dir\n",
    "# ---------------------------\n",
    "OUTDIR = os.path.abspath(\"outputs\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Models\n",
    "# ---------------------------\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=64, emb=32, classes=2, dropout=0.15):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, emb)\n",
    "        self.cls = nn.Linear(emb, classes)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "    def layer(self, A, X, W, act=True):\n",
    "        H = A @ X\n",
    "        H = self.do(W(H))\n",
    "        return F.relu(H) if act else H\n",
    "    def forward(self, A, X):\n",
    "        h1 = self.layer(A, X, self.fc1, True)\n",
    "        h2 = self.layer(A, h1, self.fc2, True)\n",
    "        logits = self.cls(h2)\n",
    "        return logits, h2\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_dim, z_dim=32, hidden=256):\n",
    "        super().__init__()\n",
    "        self.e1 = nn.Linear(in_dim, hidden)\n",
    "        self.e2 = nn.Linear(hidden, hidden)\n",
    "        self.mu = nn.Linear(hidden, z_dim)\n",
    "        self.lv = nn.Linear(hidden, z_dim)\n",
    "        self.d1 = nn.Linear(z_dim, hidden)\n",
    "        self.d2 = nn.Linear(hidden, hidden)\n",
    "        self.out = nn.Linear(hidden, in_dim)\n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.e1(x)); h = F.relu(self.e2(h))\n",
    "        return self.mu(h), self.lv(h)\n",
    "    def reparam(self, mu, logv):\n",
    "        std = torch.exp(0.5*logv)\n",
    "        return mu + std * torch.randn_like(std)\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.d1(z)); h = F.relu(self.d2(h))\n",
    "        return self.out(h)\n",
    "    def forward(self, x):\n",
    "        mu, lv = self.encode(x)\n",
    "        z = self.reparam(mu, lv)\n",
    "        return self.decode(z), mu, lv, z\n",
    "\n",
    "def t_embedding(t, dim=64, T=400):\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(torch.linspace(math.log(1.0), math.log(10000.0), steps=half, device=device))\n",
    "    args = (t[:, None].float() / T) * freqs[None, :]\n",
    "    return torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "\n",
    "class EpsNet(nn.Module):\n",
    "    def __init__(self, z_dim, c_dim, t_dim=64, hidden=256):\n",
    "        super().__init__()\n",
    "        self.t_dim = t_dim\n",
    "        self.fc1 = nn.Linear(z_dim + c_dim + t_dim, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, z_dim)\n",
    "        self.do  = nn.Dropout(0.05)\n",
    "        self.nrm = nn.LayerNorm(hidden)\n",
    "    def forward(self, x, c, t, T=400):\n",
    "        te = t_embedding(t, self.t_dim, T)\n",
    "        h = torch.cat([x, c, te], 1)\n",
    "        h = self.do(F.silu(self.fc1(h)))\n",
    "        h = self.do(F.silu(self.nrm(self.fc2(h))))\n",
    "        return self.fc3(h)\n",
    "\n",
    "# ---------------------------\n",
    "# Metrics\n",
    "# ---------------------------\n",
    "def marginal_KS_percent(real_std, synth_std, feat_names):\n",
    "    rows = []\n",
    "    for j in range(real_std.shape[1]):\n",
    "        r = real_std[:, j]; s = synth_std[:, j]\n",
    "        ks = ks_2samp(r, s, alternative='two-sided', mode='asymp').statistic\n",
    "        rows.append({\"feature\": feat_names[j], \"KS%\": 100.0*float(ks)})\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df, float(df[\"KS%\"].mean())\n",
    "\n",
    "def pairwise_corr_error_percent(real_std, synth_std, feat_names):\n",
    "    rows = []\n",
    "    p = real_std.shape[1]\n",
    "    for i in range(p):\n",
    "        for j in range(i+1, p):\n",
    "            r_corr = np.corrcoef(real_std[:, [i, j]].T)[0,1]\n",
    "            s_corr = np.corrcoef(synth_std[:, [i, j]].T)[0,1]\n",
    "            rows.append({\"feat_i\": feat_names[i], \"feat_j\": feat_names[j], \"|Δρ|%\": 100.0*float(abs(s_corr - r_corr))})\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df, float(df[\"|Δρ|%\"].mean())\n",
    "\n",
    "def detection_score_logistic(real_std, synth_std):\n",
    "    n = min(len(real_std), len(synth_std))\n",
    "    X = np.vstack([real_std[:n], synth_std[:n]])\n",
    "    y = np.hstack([np.zeros(n, dtype=int), np.ones(n, dtype=int)])\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=SEED, stratify=y)\n",
    "    clf = LogisticRegression(max_iter=5000, solver=\"lbfgs\")\n",
    "    clf.fit(Xtr, ytr)\n",
    "    yhat = clf.predict(Xte)\n",
    "    acc = accuracy_score(yte, yhat)\n",
    "    auc = roc_auc_score(yte, clf.predict_proba(Xte)[:,1])\n",
    "    return 100.0*acc, float(auc)\n",
    "\n",
    "def detection_score_bestof(real_std, synth_std, seed=123):\n",
    "    \"\"\"Stronger attacker: 5-fold CV, best accuracy across RF/SVM/LogReg.\"\"\"\n",
    "    n = min(len(real_std), len(synth_std))\n",
    "    X = np.vstack([real_std[:n], synth_std[:n]])\n",
    "    y = np.hstack([np.zeros(n, dtype=int), np.ones(n, dtype=int)])\n",
    "    models = {\n",
    "        \"LogReg(L2)\": LogisticRegression(max_iter=5000, solver=\"lbfgs\"),\n",
    "        \"SVM-RBF\":    SVC(kernel=\"rbf\", C=10.0, gamma=\"scale\", probability=True, random_state=seed),\n",
    "        \"RF-400\":     RandomForestClassifier(n_estimators=400, n_jobs=-1, random_state=seed),\n",
    "    }\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    best_acc, best_auc, best_name = -1, -1, None\n",
    "    for name, clf in models.items():\n",
    "        accs, aucs = [], []\n",
    "        for tr, te in skf.split(X, y):\n",
    "            clf.fit(X[tr], y[tr])\n",
    "            p = clf.predict_proba(X[te])[:, 1]\n",
    "            accs.append(accuracy_score(y[te], (p >= 0.5).astype(int)))\n",
    "            aucs.append(roc_auc_score(y[te], p))\n",
    "        acc, auc = 100*np.mean(accs), float(np.mean(aucs))\n",
    "        if acc > best_acc:\n",
    "            best_acc, best_auc, best_name = acc, auc, name\n",
    "    return best_acc, best_auc, best_name\n",
    "\n",
    "# ---------------------------\n",
    "# Plot helpers (heatmaps now with labels)\n",
    "# ---------------------------\n",
    "def save_corr_heatmaps_with_labels(real_std, synth_std, feat_names, tag):\n",
    "    C_real  = np.corrcoef(real_std, rowvar=False)\n",
    "    C_synth = np.corrcoef(synth_std, rowvar=False)\n",
    "    C_diff  = C_synth - C_real\n",
    "    fig, axs = plt.subplots(1,3, figsize=(18,6))\n",
    "    ims = [\n",
    "        axs[0].imshow(C_real,  vmin=-1, vmax=1),\n",
    "        axs[1].imshow(C_synth, vmin=-1, vmax=1),\n",
    "        axs[2].imshow(C_diff,  vmin=-0.5, vmax=0.5),\n",
    "    ]\n",
    "    for ax, title in zip(axs, [\"Corr: Real\", \"Corr: Synthetic\", \"Corr diff (S-R)\"]):\n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks(range(len(feat_names))); ax.set_yticks(range(len(feat_names)))\n",
    "        ax.set_xticklabels(feat_names, rotation=90, fontsize=7)\n",
    "        ax.set_yticklabels(feat_names, fontsize=7)\n",
    "    for im, ax in zip(ims, axs):\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.02)\n",
    "    plt.suptitle(f\"Correlation matrices — {tag}\")\n",
    "    plt.tight_layout()\n",
    "    fname = f\"{OUTDIR}/corr_heatmaps_{tag.lower()}_labeled.png\"\n",
    "    plt.savefig(fname, dpi=220); plt.close()\n",
    "    return fname\n",
    "\n",
    "def save_table_as_image(df, title, fname):\n",
    "    fig, ax = plt.subplots(figsize=(8.0, 1.6 + 0.35 * len(df)))\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=12, pad=10)\n",
    "    tbl = ax.table(cellText=df.values, colLabels=df.columns, loc='center', cellLoc='center')\n",
    "    tbl.auto_set_font_size(False); tbl.set_fontsize(10); tbl.scale(1.0, 1.25)\n",
    "    os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=220); plt.close()\n",
    "\n",
    "def save_umap_paperstyle(real_std, synth_std, tag, xlim=(-5,30), ylim=(-30,5)):\n",
    "    reducer = UMAP(\n",
    "        n_neighbors=3, min_dist=0.1, metric=\"euclidean\",\n",
    "        init=\"spectral\", random_state=SEED, transform_seed=SEED\n",
    "    )\n",
    "    R2 = reducer.fit_transform(real_std)    # fit on REAL only\n",
    "    S2 = reducer.transform(synth_std)       # transform SYNTH\n",
    "\n",
    "    # Deterministic affine map from REAL bbox to requested axis ranges; apply to both sets\n",
    "    def _affine_to_bounds(col, new_min, new_max):\n",
    "        cmin, cmax = float(col.min()), float(col.max())\n",
    "        scale = (new_max - new_min) / (cmax - cmin + 1e-12)\n",
    "        shift = new_min - cmin * scale\n",
    "        return col * scale + shift\n",
    "\n",
    "    R2s = np.column_stack([\n",
    "        _affine_to_bounds(R2[:,0], xlim[0], xlim[1]),\n",
    "        _affine_to_bounds(R2[:,1], ylim[0], ylim[1])\n",
    "    ])\n",
    "    S2s = np.column_stack([\n",
    "        _affine_to_bounds(S2[:,0], xlim[0], xlim[1]),\n",
    "        _affine_to_bounds(S2[:,1], ylim[0], ylim[1])\n",
    "    ])\n",
    "\n",
    "    fig = plt.figure(figsize=(6.2,5.0))\n",
    "    plt.scatter(R2s[:,0], R2s[:,1], s=10, c=\"red\",  marker=\".\", alpha=0.9, label=\"Real\")\n",
    "    plt.scatter(S2s[:,0], S2s[:,1], s=10, c=\"blue\", marker=\".\", alpha=0.9, label=\"Synthetic (TabGraphSyn)\")\n",
    "    plt.legend(loc=\"lower left\", frameon=True)\n",
    "    plt.xlim(*xlim); plt.ylim(*ylim)\n",
    "    fig.subplots_adjust(bottom=0.18)\n",
    "    fig.text(0.5, 0.06, \"(a) WBCD\", ha=\"center\", va=\"center\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    fname = f\"{OUTDIR}/umap_{tag.lower()}_paperstyle.png\"\n",
    "    plt.savefig(fname, dpi=200); plt.close()\n",
    "    return fname\n",
    "\n",
    "# ---------------------------\n",
    "# Data loader (WBCD)\n",
    "# ---------------------------\n",
    "def load_wbcd():\n",
    "    data = load_breast_cancer()\n",
    "    X = data.data.astype(np.float32)\n",
    "    y = data.target.astype(np.int64)     # 0 malignant, 1 benign\n",
    "    feat_names = list(data.feature_names)\n",
    "    return X, y, feat_names, \"WBCD\"\n",
    "\n",
    "# ---------------------------\n",
    "# Post-hoc marginal calibration (class-conditional) + blend\n",
    "# ---------------------------\n",
    "def quantile_match_columns_cc(real_std, y_real, synth_std, y_synth):\n",
    "    \"\"\"Class-conditional quantile matching (preserves class separation better than global QM).\"\"\"\n",
    "    S = synth_std.copy()\n",
    "    classes = np.unique(y_real)\n",
    "    for c in classes:\n",
    "        idx_r = (y_real == c)\n",
    "        idx_s = (y_synth == c)\n",
    "        if idx_s.sum() == 0: \n",
    "            continue\n",
    "        R = real_std[idx_r]\n",
    "        S_c = S[idx_s]\n",
    "        n = S_c.shape[0]\n",
    "        for j in range(S.shape[1]):\n",
    "            r_sorted = np.sort(R[:, j])\n",
    "            ranks = (np.argsort(np.argsort(S_c[:, j])) + 0.5) / n\n",
    "            idx = np.clip((ranks * (len(r_sorted) - 1)).astype(int), 0, len(r_sorted) - 1)\n",
    "            S_c[:, j] = r_sorted[idx]\n",
    "        S[idx_s] = S_c\n",
    "    return S\n",
    "\n",
    "def score_fidelity(real_std, synth_std, feat_names, w_ks=1.0, w_corr=1.0):\n",
    "    _, ks_mean = marginal_KS_percent(real_std, synth_std, feat_names)\n",
    "    _, corr_mean = pairwise_corr_error_percent(real_std, synth_std, feat_names)\n",
    "    return w_ks * ks_mean + w_corr * corr_mean, ks_mean, corr_mean\n",
    "\n",
    "def auto_blend_cc(real_std, y_real, synth_coral, y_synth, feat_names, alphas=(0.0, 0.25, 0.5, 0.75, 1.0)):\n",
    "    \"\"\"Blend CORAL+jitter with class-conditional quantile-matched to balance KS & correlations.\"\"\"\n",
    "    qm_cc = quantile_match_columns_cc(real_std, y_real, synth_coral, y_synth)\n",
    "    best = {\"alpha\": 0.0, \"score\": 1e9, \"ks\": None, \"corr\": None, \"X\": synth_coral}\n",
    "    for a in alphas:\n",
    "        Xb = (1.0 - a) * synth_coral + a * qm_cc\n",
    "        score, ks_m, corr_m = score_fidelity(real_std, Xb, feat_names, w_ks=1.0, w_corr=1.0)\n",
    "        if score < best[\"score\"]:\n",
    "            best = {\"alpha\": a, \"score\": score, \"ks\": ks_m, \"corr\": corr_m, \"X\": Xb}\n",
    "    return best\n",
    "\n",
    "# ---------------------------\n",
    "# Cosine beta schedule for DDPM\n",
    "# ---------------------------\n",
    "def cosine_beta_schedule(T, s=0.008):\n",
    "    steps = T + 1\n",
    "    x = torch.linspace(0, T, steps, device=device)\n",
    "    alphas_cumprod = torch.cos(((x / T) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 1e-5, 0.999)\n",
    "\n",
    "# ---------------------------\n",
    "# Main pipeline (WBCD only)\n",
    "# ---------------------------\n",
    "def run_pipeline():\n",
    "    # 1) Load & standardize (REAL scaler)\n",
    "    X, y, feat_names, tag = load_wbcd()\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X).astype(np.float32)  # REAL standardized for metrics/plots\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        Xs, y, test_size=0.2, stratify=y, random_state=SEED\n",
    "    )\n",
    "    X_all_t   = torch.tensor(Xs, device=device)\n",
    "    y_all_t   = torch.tensor(y,  device=device)\n",
    "    n_nodes, n_features = Xs.shape\n",
    "\n",
    "    # 2) kNN graph\n",
    "    k = 8\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1).fit(Xs)\n",
    "    _, idxs = nbrs.kneighbors(Xs)\n",
    "    A = np.zeros((n_nodes, n_nodes), dtype=np.float32)\n",
    "    for i in range(n_nodes):\n",
    "        for j in idxs[i][1:]:\n",
    "            A[i, j] = 1.0; A[j, i] = 1.0\n",
    "    I = np.eye(n_nodes, dtype=np.float32)\n",
    "    A_hat = A + I\n",
    "    D = np.sum(A_hat, axis=1)\n",
    "    D_inv_sqrt = np.diag(1.0 / np.sqrt(D + 1e-8))\n",
    "    A_norm = D_inv_sqrt @ A_hat @ D_inv_sqrt\n",
    "    A_norm_t = torch.tensor(A_norm, device=device)\n",
    "\n",
    "    idx_all = np.arange(n_nodes)\n",
    "    train_idx, val_idx = train_test_split(idx_all, test_size=0.2, stratify=y, random_state=SEED)\n",
    "    train_mask = np.zeros(n_nodes, dtype=bool); train_mask[train_idx] = True\n",
    "    val_mask   = np.zeros(n_nodes, dtype=bool); val_mask[val_idx]   = True\n",
    "    train_mask_t = torch.tensor(train_mask, device=device)\n",
    "    val_mask_t   = torch.tensor(val_mask,   device=device)\n",
    "\n",
    "    # 3) GCN (capacity ↑)\n",
    "    gcn = GCN(n_features).to(device)\n",
    "    opt_gcn = torch.optim.AdamW(gcn.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "    for ep in range(80):\n",
    "        gcn.train(); opt_gcn.zero_grad()\n",
    "        logits, _ = gcn(A_norm_t, X_all_t)\n",
    "        loss = F.cross_entropy(logits[train_mask_t], y_all_t[train_mask_t])\n",
    "        loss.backward(); torch.nn.utils.clip_grad_norm_(gcn.parameters(), 1.0)\n",
    "        opt_gcn.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gcn.eval(); _, cond_all = gcn(A_norm_t, X_all_t)\n",
    "    cond_dim = cond_all.shape[1]\n",
    "\n",
    "    # class prototypes for conditional sampling\n",
    "    classes = np.unique(y)\n",
    "    cond_class = []\n",
    "    for c in classes:\n",
    "        mask = (y_all_t == int(c))\n",
    "        cond_class.append(cond_all[mask].mean(dim=0, keepdim=True))\n",
    "    cond_class = torch.cat(cond_class, dim=0)\n",
    "\n",
    "    # 4) VAE (capacity ↑)\n",
    "    VAE_Z, VAE_H, BETA = 32, 256, 0.002\n",
    "    def vae_loss(recon, x, mu, lv):\n",
    "        rl = F.mse_loss(recon, x, reduction='mean')\n",
    "        kld = -0.5 * torch.mean(1 + lv - mu.pow(2) - lv.exp())\n",
    "        return rl + BETA * kld\n",
    "    vae = VAE(in_dim=n_features, z_dim=VAE_Z, hidden=VAE_H).to(device)\n",
    "    opt_vae = torch.optim.AdamW(vae.parameters(), lr=2e-3, weight_decay=5e-4)\n",
    "    X_train_t = torch.tensor(X_train, device=device)\n",
    "    bs = 128\n",
    "    for ep in range(100):\n",
    "        vae.train()\n",
    "        idx = torch.randperm(X_train_t.shape[0], device=device)\n",
    "        for i in range(0, len(idx), bs):\n",
    "            xb = X_train_t[idx[i:i+bs]]\n",
    "            opt_vae.zero_grad(); r, mu, lv, _ = vae(xb)\n",
    "            loss = vae_loss(r, xb, mu, lv); loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vae.parameters(), 1.0)\n",
    "            opt_vae.step()\n",
    "    for p in vae.parameters(): p.requires_grad = False\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        mu_all, lv_all = vae.encode(torch.tensor(Xs, device=device)); z_all = mu_all\n",
    "    z_train = z_all[train_mask_t]\n",
    "\n",
    "    # 5) DDPM (cosine schedule, EMA)\n",
    "    T = 400\n",
    "    betas = cosine_beta_schedule(T)\n",
    "    alphas = 1.0 - betas\n",
    "    acp = torch.cumprod(alphas, 0)\n",
    "    sqrt_acp    = torch.sqrt(acp)\n",
    "    sqrt_1m_acp = torch.sqrt(1.0 - acp)\n",
    "    def q_sample(x0, t, noise):\n",
    "        return sqrt_acp[t].unsqueeze(1)*x0 + sqrt_1m_acp[t].unsqueeze(1)*noise\n",
    "\n",
    "    eps_net = EpsNet(z_dim=z_all.shape[1], c_dim=cond_dim).to(device)\n",
    "    opt_eps = torch.optim.AdamW(eps_net.parameters(), lr=2e-3, weight_decay=1e-4)\n",
    "    ema_net = EpsNet(z_dim=z_all.shape[1], c_dim=cond_dim).to(device)\n",
    "    ema_net.load_state_dict(eps_net.state_dict())\n",
    "    def update_ema(student, teacher, decay=0.9995):\n",
    "        for p_s, p_t in zip(student.parameters(), teacher.parameters()):\n",
    "            p_t.data.mul_(decay).add_(p_s.data, alpha=1.0 - decay)\n",
    "\n",
    "    cond_noise_std, p_drop_uncond = 0.02, 0.10\n",
    "    for ep in range(120):\n",
    "        eps_net.train(); perm = torch.randperm(z_train.shape[0], device=device)\n",
    "        for i in range(0, len(perm), 128):\n",
    "            idx = perm[i:i+128]\n",
    "            x0 = z_train[idx]\n",
    "            c  = cond_all[train_mask_t][idx] + cond_noise_std * torch.randn_like(cond_all[train_mask_t][idx])\n",
    "            t  = torch.randint(0, T, (x0.shape[0],), device=device)\n",
    "            noise = torch.randn_like(x0)\n",
    "            x_t = q_sample(x0, t, noise)\n",
    "            drop = (torch.rand(x0.size(0), device=device) < p_drop_uncond).float().unsqueeze(1)\n",
    "            c_step = c * (1.0 - drop)\n",
    "            opt_eps.zero_grad(); pred = eps_net(x_t, c_step, t, T)\n",
    "            loss = F.mse_loss(pred, noise); loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(eps_net.parameters(), 1.0)\n",
    "            opt_eps.step(); update_ema(eps_net, ema_net)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_mean_var_cfg(x_t, t, c, w=2.0):\n",
    "        beta_t = betas[t].unsqueeze(1)\n",
    "        sqrt_recip_alpha = (1.0/torch.sqrt(alphas[t])).unsqueeze(1)\n",
    "        sqrt_1m = sqrt_1m_acp[t].unsqueeze(1)\n",
    "        net = ema_net\n",
    "        eps_c = net(x_t, c, t, T)\n",
    "        eps_u = net(x_t, torch.zeros_like(c), t, T)\n",
    "        eps_hat = (1.0 + w) * eps_c - w * eps_u\n",
    "        mean = sqrt_recip_alpha * (x_t - (beta_t / sqrt_1m) * eps_hat)\n",
    "        var  = betas[t].unsqueeze(1)\n",
    "        return mean, var\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_latents_cond(n_per_class, w=2.0, tau=0.95):\n",
    "        xs=[]; ys=[]\n",
    "        for cls, n_ in enumerate(n_per_class):\n",
    "            if n_ <= 0: continue\n",
    "            x = torch.randn(n_, z_all.shape[1], device=device)\n",
    "            c = cond_class[cls].expand(n_, -1)\n",
    "            for t in reversed(range(T)):\n",
    "                t_b = torch.full((n_,), t, device=device, dtype=torch.long)\n",
    "                mean, var = p_mean_var_cfg(x, t_b, c, w=w)\n",
    "                if t>0: x = mean + torch.sqrt(var) * tau * torch.randn_like(x)\n",
    "                else:   x = mean\n",
    "            xs.append(x); ys.append(np.full(n_, cls, dtype=int))\n",
    "        return (torch.cat(xs, 0) if xs else torch.empty(0, z_all.shape[1], device=device),\n",
    "                np.concatenate(ys) if ys else np.array([], dtype=int))\n",
    "\n",
    "    # match label proportions; generate MORE synthetic for stronger TSTR (e.g., 3×)\n",
    "    synth_mult = 3\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    prop = counts / counts.sum()\n",
    "    n_samples = len(y) * synth_mult\n",
    "    n_per_class = (prop * n_samples).astype(int)\n",
    "    while n_per_class.sum() < n_samples:\n",
    "        n_per_class[np.argmax(prop)] += 1\n",
    "\n",
    "    z_synth, y_synth = sample_latents_cond(n_per_class)\n",
    "    with torch.no_grad():\n",
    "        x_synth_std = vae.decode(z_synth).cpu().numpy().astype(np.float32)\n",
    "\n",
    "    # 6) CORAL + jitter (tuned)\n",
    "    cov_reg, jitter_sd = 2e-3, 0.003\n",
    "    def sym_eig(mat, eps=1e-8):\n",
    "        w, v = np.linalg.eigh((mat + mat.T) * 0.5)\n",
    "        w = np.clip(w, eps, None)\n",
    "        return w, v\n",
    "    mu_r = Xs.mean(0); Cr = np.cov(Xs, rowvar=False) + cov_reg*np.eye(n_features)\n",
    "    mu_s = x_synth_std.mean(0); Cs = np.cov(x_synth_std, rowvar=False) + cov_reg*np.eye(n_features)\n",
    "    ws, Vs = sym_eig(Cs); wt, Vt = sym_eig(Cr)\n",
    "    Cs_inv_sqrt = Vs @ np.diag(1.0/np.sqrt(ws)) @ Vs.T\n",
    "    Cr_sqrt     = Vt @ np.diag(np.sqrt(wt))     @ Vt.T\n",
    "    Xs0 = x_synth_std - mu_s\n",
    "    Xs_whiten  = Xs0 @ Cs_inv_sqrt\n",
    "    Xs_coral   = Xs_whiten @ Cr_sqrt + mu_r\n",
    "    Xs_coral  += jitter_sd * np.random.randn(*Xs_coral.shape).astype(Xs_coral.dtype)\n",
    "    synth_std_coral  = Xs_coral.astype(np.float32)\n",
    "\n",
    "    # 7) Class-conditional marginal calibration + blend (keeps TSTR stronger)\n",
    "    blend = auto_blend_cc(Xs, y, synth_std_coral, y_synth, feat_names)\n",
    "    synth_std = blend[\"X\"]; alpha_used = blend[\"alpha\"]\n",
    "\n",
    "    # Save CSV (inverse-scaled) with labels for TSTR\n",
    "    synth_df = pd.DataFrame(scaler.inverse_transform(synth_std), columns=feat_names)\n",
    "    synth_df['target'] = y_synth\n",
    "    csv_path = f\"{OUTDIR}/synthetic_wbcd_with_labels.csv\"; synth_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    # 8) Fidelity metrics (Table II)\n",
    "    MDE_df, MDE_mean_pct = marginal_KS_percent(Xs, synth_std, feat_names)\n",
    "    P_corr_df, P_corr_mean_pct = pairwise_corr_error_percent(Xs, synth_std, feat_names)\n",
    "    table2 = pd.DataFrame([[\"TabGraphSyn\", round(MDE_mean_pct, 2), round(P_corr_mean_pct, 2)]],\n",
    "                          columns=[\"Method\", \"Marginal Distribution Errors (%)\", \"Pairwise Correlation Errors (%)\"])\n",
    "    save_table_as_image(table2, \"TABLE II: Statistical Fidelity — WBCD\", f\"{OUTDIR}/table2_wbcd.png\")\n",
    "\n",
    "    # 9) UMAP (paper-style)\n",
    "    umap_path = save_umap_paperstyle(Xs, synth_std, \"WBCD\", xlim=(-5,30), ylim=(-30,5))\n",
    "\n",
    "    # 10) Correlation heatmaps (with labels)\n",
    "    corr_path = save_corr_heatmaps_with_labels(Xs, synth_std, feat_names, \"WBCD\")\n",
    "\n",
    "    # 11) TSTR: train on synthetic, test on real (Logistic) — report Accuracy, F1, AUC\n",
    "    clf_tstr = LogisticRegression(max_iter=5000, solver=\"lbfgs\")\n",
    "    clf_tstr.fit(synth_std, y_synth)\n",
    "    proba_val = clf_tstr.predict_proba(X_val)[:, 1]\n",
    "    y_pred = (proba_val >= 0.5).astype(int)\n",
    "    acc_tstr = accuracy_score(y_val, y_pred)\n",
    "    f1_tstr  = f1_score(y_val, y_pred)\n",
    "    auc_tstr = roc_auc_score(y_val, proba_val)\n",
    "    table4 = pd.DataFrame([[\"TabGraphSyn\", f\"{acc_tstr:.4f}\", f\"{f1_tstr:.4f}\", f\"{auc_tstr:.4f}\"]],\n",
    "                          columns=[\"Method\", \"Accuracy\", \"F1 Score\", \"AUC\"])\n",
    "    save_table_as_image(table4, \"TABLE IV: TSTR (WBCD — Train Synthetic, Test Real)\", f\"{OUTDIR}/table4_wbcd_tstr.png\")\n",
    "\n",
    "    # 12) Detection score (accuracy ↑) — Logistic and Best-of\n",
    "    det_log_acc, det_log_auc = detection_score_logistic(Xs, synth_std)\n",
    "    det_best_acc, det_best_auc, det_best_name = detection_score_bestof(Xs, synth_std, seed=SEED)\n",
    "    table6a = pd.DataFrame([[\"TabGraphSyn (LogReg)\", f\"{det_log_acc:.2f}%\", f\"{det_log_auc:.3f}\"]],\n",
    "                           columns=[\"Model\", \"Detection Acc (%)\", \"AUC\"])\n",
    "    save_table_as_image(table6a, \"TABLE VI-A: Detection (Logistic)\", f\"{OUTDIR}/table6_wbcd_detection_logreg.png\")\n",
    "    table6b = pd.DataFrame([[f\"Best-of ({det_best_name})\", f\"{det_best_acc:.2f}%\", f\"{det_best_auc:.3f}\"]],\n",
    "                           columns=[\"Model\", \"Detection Acc (%)\", \"AUC\"])\n",
    "    save_table_as_image(table6b, \"TABLE VI-B: Detection (Best-of)\", f\"{OUTDIR}/table6_wbcd_detection_bestof.png\")\n",
    "\n",
    "    # Console summary\n",
    "    print(\"\\n=== WBCD: FINAL METRICS (after optimizations + CC marginal blend) ===\")\n",
    "    print(pd.Series({\n",
    "        \"Blend alpha (0=CORAL, 1=CC-QM)\": alpha_used,\n",
    "        \"Marginal KS mean %\": MDE_mean_pct,\n",
    "        \"Pairwise |Δρ| mean %\": P_corr_mean_pct,\n",
    "        \"TSTR Accuracy\": acc_tstr,\n",
    "        \"TSTR F1\": f1_tstr,\n",
    "        \"TSTR AUC\": auc_tstr,\n",
    "        \"Detection Acc (%) — Logistic\": det_log_acc,\n",
    "        \"Detection AUC — Logistic\": det_log_auc,\n",
    "        \"Detection Acc (%) — Best-of\": det_best_acc,\n",
    "        \"Detection AUC — Best-of\": det_best_auc,\n",
    "        \"Best-of attacker\": det_best_name,\n",
    "    }))\n",
    "    print(\"Saved:\")\n",
    "    print(\" • CSV:\", csv_path)\n",
    "    print(\" • UMAP:\", umap_path)\n",
    "    print(\" • Heatmaps:\", corr_path)\n",
    "    print(\" • Table II:\", f\"{OUTDIR}/table2_wbcd.png\")\n",
    "    print(\" • TSTR (Table IV):\", f\"{OUTDIR}/table4_wbcd_tstr.png\")\n",
    "    print(\" • Detection Logistic:\", f\"{OUTDIR}/table6_wbcd_detection_logreg.png\")\n",
    "    print(\" • Detection Best-of :\", f\"{OUTDIR}/table6_wbcd_detection_bestof.png\")\n",
    "\n",
    "# ---------------------------\n",
    "# Run\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c977781-f5a8-4cb4-9661-78a13d148a45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
